# MediaPipe face mesh rendering subgraph.

type: "FaceRendererCpu"

# CPU image. (ImageFrame)
input_stream: "IMAGE:input_image"

# Regions of interest calculated based on palm detections.
# (std::vector<NormalizedRect>)
input_stream: "NORM_RECTS:rects"

# Multi-face Alignment data
# (std::vector<std::map<std::string, double>>)
input_stream: "ALIGNMENTS:multi_face_alignments"

# Multi-face blink data
# (std::vector<std::map<std::string, double>>)
input_stream: "BLINKS:multi_face_blinks"

# CPU image with rendered data. (ImageFrame)
output_stream: "IMAGE:output_image"

node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE:input_image"
  output_stream: "SIZE:image_size"
}

# Converts normalized rects to drawing primitives for annotation overlay.
node {
  calculator: "RectToRenderDataCalculator"
  input_stream: "NORM_RECTS:rects"
  output_stream: "RENDER_DATA:rects_render_data"
  node_options: {
    [type.googleapis.com/mediapipe.RectToRenderDataCalculatorOptions] {
      filled: false
      color { r: 255 g: 0 b: 0 }
      thickness: 4.0
    }
  }
}

# Convert the FaceAlignment to RenderData
node {
  calculator: "FaceAlignmentToRenderDataCalculator"
  input_stream: "ALIGNMENT:multi_face_alignments"
  output_stream: "RENDER:alignment_render_data"
}

# Converts detected eye blinks to drawing primitives for annotation overlay.
node {
  calculator: "EyeBlinkToRenderDataCalculator"
  input_stream: "BLINK:multi_face_blinks"
  output_stream: "RENDER:blink_render_data"
}

# Draws annotations and overlays them on top of the input images.
node {
  calculator: "AnnotationOverlayCalculator"
  input_stream: "IMAGE:input_image"
  input_stream: "alignment_render_data"
  input_stream: "blink_render_data"
  input_stream: "rects_render_data"

  # By pass the unbounded wait in case no render data (i.e. No face detected)
  # input_stream_handler {
  #  input_stream_handler: "ImmediateInputStreamHandler"
  # }

  output_stream: "IMAGE:output_image"
}
